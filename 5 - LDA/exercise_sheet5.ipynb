{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key savefig.frameon in file /home/michele/miniconda3/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 421 ('savefig.frameon : True')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key verbose.level in file /home/michele/miniconda3/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 472 ('verbose.level  : silent      # one of silent, helpful, debug, debug-annoying')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key verbose.fileo in file /home/michele/miniconda3/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 473 ('verbose.fileo  : sys.stdout  # a log filename, sys.stdout or sys.stderr')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "In /home/michele/miniconda3/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/michele/miniconda3/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/michele/miniconda3/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /home/michele/miniconda3/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/michele/miniconda3/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/michele/miniconda3/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/michele/miniconda3/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/michele/miniconda3/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "################################################################################"
   },
   "source": [
    "# SNLP exercise sheet 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from glob import glob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "from string import punctuation\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class LdaModel(object):\n",
    "    # add some instance variables for storing the corpus\n",
    "    \n",
    "    \n",
    "    # Exercise 1 ###################################################################\n",
    "    def __init__(self, path_to_corpus, k):\n",
    "        '''\n",
    "        Import and preprocess the corpus.\n",
    "        Parameters: path_to_corpus: string; path to the directory containing the corpus\n",
    "        '''\n",
    "\n",
    "        # list of tokenized documents (list of lists of tokens)\n",
    "        self.corpus = []\n",
    "        # list of topics for document (list of lists of topics)\n",
    "        self.topics = []\n",
    "        # total number of tokens in the whole corpus\n",
    "        self.N = 0\n",
    "        # number of topics\n",
    "        self.k = k\n",
    "\n",
    "        self.words_index = {}\n",
    "\n",
    "        # counts\n",
    "        self.topic_word_counts = None\n",
    "        self.document_topic_counts = None\n",
    "        self.topic_count = None\n",
    "\n",
    "        # preprocess all documents in the corpus\n",
    "        self.filenames = []\n",
    "        for newsgroup_dir in glob(join(path_to_corpus, '*')):\n",
    "            for filename in glob(join(newsgroup_dir, '*')):\n",
    "                self.filenames.append(filename)\n",
    "\n",
    "        # cut corpus to help developement\n",
    "        self.filenames = self.filenames[:int(len(self.filenames) * 0.01)]\n",
    "\n",
    "        for filename in tqdm(self.filenames, desc='importing corpus'):\n",
    "            try:\n",
    "                self.corpus.append(self._preprocess(filename))\n",
    "            except Exception as e:\n",
    "                print(filename, 'error', e)\n",
    "\n",
    "        words = set()\n",
    "        # initialize document topics randomly\n",
    "        for document in self.corpus:\n",
    "            document_topics = []\n",
    "            for token in document:\n",
    "                self.N += 1\n",
    "                words.add(token)\n",
    "                document_topics.append(random.randint(0, self.k-1))\n",
    "            self.topics.append(document_topics)\n",
    "\n",
    "        # assign an index to words\n",
    "        for i, word in enumerate(words):\n",
    "            self.words_index[word] = i\n",
    "\n",
    "        # topic count\n",
    "        self.topic_count = np.zeros(20)\n",
    "        # topics on the rows, words on the columns\n",
    "        self.topic_word_counts = np.zeros((self.k, len(self.words_index)))\n",
    "        # documents on the rows, topics on the columns\n",
    "        self.document_topic_counts = np.zeros((len(self.corpus), self.k))\n",
    "\n",
    "        for i, (document_words, document_topics) in tqdm(enumerate(zip(self.corpus, self.topics)), desc='inizializing structures'):\n",
    "            for word, topic in zip(document_words, document_topics):\n",
    "                self.topic_count[topic] += 1\n",
    "                self.topic_word_counts[topic, self.words_index[word]] += 1\n",
    "                self.document_topic_counts[i, topic] += 1\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _preprocess(filename):\n",
    "        with open(filename) as infile:\n",
    "            data = [line.strip() for line in infile]\n",
    "\n",
    "        text = []\n",
    "        header_ended = False\n",
    "        for line in data:\n",
    "            # extract the subject\n",
    "            if line.lower().startswith('subject: '):\n",
    "                text.append(line.split(': ', 1)[1])\n",
    "            # detect end of header\n",
    "            if line == '':\n",
    "                header_ended = True\n",
    "            # get message lines\n",
    "            if header_ended and line != '':\n",
    "                text.append(line)\n",
    "\n",
    "        # join in a single string\n",
    "        text = '\\n'.join(text)\n",
    "        # tokenize the string\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "\n",
    "        # remove stopwords\n",
    "        stopwords_set = set(stopwords.words('english'))\n",
    "        tokens = list(filter(lambda token: token.lower() not in stopwords_set, tokens))\n",
    "\n",
    "        # remove punctuation\n",
    "        tokens = list(filter(lambda token: token not in punctuation, tokens))\n",
    "\n",
    "        return tokens\n",
    "    \n",
    "    # Exercise 2 ###################################################################\n",
    "    def gibbs_sampling(self, num_iterations, alpha=0.25, beta=0.1):\n",
    "        '''\n",
    "        Implement the LDA Gibbs sampling algorithm.\n",
    "        Parameters: num_iterations: int; number of sampling steps to do for each word\n",
    "                    alpha: float; alpha parameter of the Dirichlet prior distribution\n",
    "                    beta: float; beta parameter of the Dirichlet prior distribution\n",
    "        '''\n",
    "        \n",
    "        for _ in range(num_iterations):\n",
    "            for i, (document_words, document_topics) in tqdm(enumerate(zip(self.corpus, self.topics)), desc='documents'):\n",
    "                for j, (word, topic) in tqdm(enumerate(zip(document_words, document_topics)), desc='words'):\n",
    "                    self.document_topic_counts[i, topic] -= 1\n",
    "                    self.topic_word_counts[topic, self.words_index[word]] -= 1\n",
    "                    self.topic_count[topic] -= 1\n",
    "\n",
    "                    distribution = []\n",
    "                    for k in range(self.k):\n",
    "                        normalization = sum(self.topic_word_counts[k,:]) + beta * len(self.words_index)\n",
    "                        p_zk = (self.document_topic_counts[i,k] + alpha) *\\\n",
    "                               (self.topic_word_counts[k,self.words_index[word]] + beta) /\\\n",
    "                               normalization\n",
    "                        distribution.append(p_zk)\n",
    "\n",
    "                    new_topic = random.choices(range(self.k), distribution)\n",
    "                    # assign new topic\n",
    "                    document_topics[j] = new_topic\n",
    "                    # update counts\n",
    "                    self.topic_count[new_topic] += 1\n",
    "                    self.topic_word_counts[new_topic, self.words_index[word]] += 1\n",
    "                    self.document_topic_counts[i, new_topic] += 1\n",
    "\n",
    "    def print_predictions(self):\n",
    "        for document_name, document_topic_count in zip(self.filenames, self.document_topic_counts):\n",
    "            print(f'File: \"{document_name}\", topic: {document_topic_count.argmin()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Benediktine', 'Metaphysics', 'Benedikt', 'Rosenau', 'writes', 'great', 'authority', 'CONTRADICTORY', 'EXIST', '``', 'Contradictory', \"''\", 'property', 'language', 'correct', 'THINGS', 'DEFINED', 'CONTRADICTORY', 'LANGUAGE', 'EXIST', 'object', 'definitions', 'reality', 'amend', 'THINGS', 'DESCRIBED', 'CONTRADICTORY', 'LANGUAGE', 'EXIST', \"'ve\", 'come', 'something', 'plainly', 'false', 'Failures', 'description', 'merely', 'failures', 'description', \"'m\", 'objectivist', 'remember', '--', 'C.', 'Wingate', '``', 'peace', 'God', 'peace', 'strife', 'closed', 'sod', 'mangoe', 'cs.umd.edu', 'Yet', 'brothers', 'pray', 'one', 'thing', 'tove', 'mangoe', \"marv'lous\", 'peace', 'God', \"''\"]\n"
     ]
    }
   ],
   "source": [
    "# test preprocessing\n",
    "print(LdaModel._preprocess('corpus/alt.atheism/51267'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e8e82102f74847b8823c9222556cee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='importing corpus'), FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus/rec.sport.baseball/104352 error 'utf-8' codec can't decode byte 0xd1 in position 1658: invalid continuation byte\n",
      "corpus/talk.religion.misc/83651 error 'utf-8' codec can't decode byte 0xb6 in position 1191: invalid start byte\n",
      "corpus/comp.sys.ibm.pc.hardware/60366 error 'utf-8' codec can't decode byte 0xfe in position 572: invalid start byte\n",
      "corpus/comp.sys.mac.hardware/51917 error 'utf-8' codec can't decode byte 0xe4 in position 255: invalid continuation byte\n",
      "corpus/comp.sys.mac.hardware/51892 error 'utf-8' codec can't decode byte 0xb2 in position 407: invalid start byte\n",
      "corpus/comp.sys.mac.hardware/51904 error 'utf-8' codec can't decode byte 0xb5 in position 1546: invalid start byte\n",
      "corpus/comp.sys.mac.hardware/51865 error 'utf-8' codec can't decode byte 0xe4 in position 313: invalid continuation byte\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "030800b8c123434192316bbf0f49ce49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='inizializing structures'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize model\n",
    "lda = LdaModel('corpus', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1993\n",
      "66744\n",
      "[17835. 17887. 17937. 18059. 17999. 18040. 18105. 17942. 18109. 17961.\n",
      " 17781. 18067. 18001. 18047. 18015. 17788. 18029. 17973. 17740. 18277.]\n",
      "[[1. 0. 0. ... 1. 0. 0.]\n",
      " [1. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]]\n",
      "[[ 4.  5.  2. ...  4.  4.  1.]\n",
      " [18. 16. 21. ... 11. 13. 12.]\n",
      " [ 3.  1.  3. ...  2.  2.  2.]\n",
      " ...\n",
      " [ 7.  4.  5. ...  9.  5.  9.]\n",
      " [ 6.  8. 11. ... 11.  7.  7.]\n",
      " [ 3.  1.  3. ...  0.  4.  4.]]\n"
     ]
    }
   ],
   "source": [
    "print(len(lda.corpus))\n",
    "print(len(lda.words_index))\n",
    "print(lda.topic_count)\n",
    "print(lda.topic_word_counts)\n",
    "print(lda.document_topic_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e872656781b48b3a71785f82d2ee1d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='documents'), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "904ad0ee3ec147d38903f3d50ba8ebe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='words'), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), ma…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "235dc5ca61c64125938d1fe84bb289b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='words'), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), ma…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1% of the corpus is considered to speed the test\n",
    "# uncomment the line in the constructor to work on the full corpus\n",
    "lda.gibbs_sampling(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lda.print_predictions()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}