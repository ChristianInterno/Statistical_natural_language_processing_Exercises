{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key savefig.frameon in file /home/michele/miniconda3/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 421 ('savefig.frameon : True')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key verbose.level in file /home/michele/miniconda3/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 472 ('verbose.level  : silent      # one of silent, helpful, debug, debug-annoying')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key verbose.fileo in file /home/michele/miniconda3/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 473 ('verbose.fileo  : sys.stdout  # a log filename, sys.stdout or sys.stderr')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "In /home/michele/miniconda3/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/michele/miniconda3/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/michele/miniconda3/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /home/michele/miniconda3/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/michele/miniconda3/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/michele/miniconda3/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/michele/miniconda3/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/michele/miniconda3/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "################################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "# SNLP exercise sheet 4\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import log, inf\n",
    "import sys\n",
    "import numpy as np\n",
    "from collections import OrderedDict, defaultdict\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "'''",
    "lines_to_next_cell": 0
   },
   "source": [
    "This function can be used for importing the corpus.\n",
    "Parameters: path_to_file: string; path to the file containing the corpus\n",
    "Returns: list of list; the first layer list contains the sentences of the corpus;\n",
    "    the second layer list contains tuples (token,label) representing a labelled sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_corpus(path_to_file):\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    f = open(path_to_file)\n",
    "\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line: break\n",
    "\n",
    "        line = line.strip()\n",
    "        if len(line) == 0:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "            continue\n",
    "\n",
    "        parts = line.split(' ')\n",
    "        sentence.append((parts[0], parts[-1]))\n",
    "\n",
    "    f.close()\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'l1=start/l2=NN', 't=cat/l=NN'}\n"
     ]
    }
   ],
   "source": [
    "def orderedDict2array(orderedDict):\n",
    "    return np.array(list(orderedDict.values()))\n",
    "\n",
    "def full_log(x):\n",
    "    if x == 0:\n",
    "        return -inf\n",
    "    else:\n",
    "        return log(x)\n",
    "\n",
    "def build_features(token, label, prev_label):\n",
    "    features = set()\n",
    "    features.add('t=' + token + '/l=' + label)\n",
    "    features.add('l1=' + prev_label + '/l2=' + label)\n",
    "    return features\n",
    "\n",
    "print(build_features('cat', 'NN', 'start'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearChainCRF(object):\n",
    "    # training corpus\n",
    "    corpus = None\n",
    "    \n",
    "    # (numpy) array containing the parameters of the model\n",
    "    # has to be initialized by the method 'initialize'\n",
    "    # An OrderedDict can be casted to a numpy array, but we can use strings to access its elements\n",
    "    theta = OrderedDict()\n",
    "    \n",
    "    # set containing all features observed in the corpus 'self.corpus'\n",
    "    # choose an appropriate data structure for representing features\n",
    "    # each element of this set has to be assigned to exactly one component of the vector 'self.theta'\n",
    "    features = set()\n",
    "    \n",
    "    # set containing all lables observed in the corpus 'self.corpus'\n",
    "    labels = set()\n",
    "    \n",
    "    \n",
    "    def initialize(self, corpus):\n",
    "        '''\n",
    "        build two sets 'self.features' and 'self.labels'\n",
    "        '''\n",
    "        self.corpus = corpus\n",
    "        \n",
    "        # add starting label\n",
    "        self.labels.add('start')\n",
    "        \n",
    "        # extract all tokens and labels\n",
    "        for sentence in corpus:\n",
    "            \n",
    "            prev_label = 'start'\n",
    "            for token, label in sentence:\n",
    "                \n",
    "                self.labels.add(label)\n",
    "                \n",
    "                features = build_features(token, label, prev_label)\n",
    "                for feature in features:\n",
    "                    self.features.add(feature)\n",
    "                    \n",
    "                prev_label = label\n",
    "                \n",
    "        # initialize theta\n",
    "        # sorted is used to establish a clear order in the feature set\n",
    "        for feature in sorted(self.features):\n",
    "            self.theta[feature] = 1\n",
    "            \n",
    "    #\n",
    "    def get_active_features(self, token, label, prev_label):\n",
    "        observed_features = build_features(token, label, prev_label)\n",
    "        return observed_features.intersection(self.features)\n",
    "    \n",
    "    #\n",
    "    def compute_psi(self, token, label, prev_label):\n",
    "        \n",
    "        param_sum = 0.0\n",
    "        for feature in self.get_active_features(token, label, prev_label):\n",
    "            param_sum += self.theta[feature]\n",
    "            \n",
    "        return math.exp(param_sum)\n",
    "\n",
    "    # Exercise 1 a) ###################################################################\n",
    "    def forward_variables(self, sentence):\n",
    "        '''\n",
    "        Compute the forward variables for a given sentence.\n",
    "        Parameters: sentence: list of strings representing a sentence.\n",
    "        Returns: data structure containing the matrix of forward variables\n",
    "        '''\n",
    "        \n",
    "        T = len(sentence)\n",
    "        # alpha is indexed from 0 to T - 1 to be consistent with the array indexing\n",
    "        alpha = {}        \n",
    "        \n",
    "        # initialization\n",
    "        # this dictionary contains labels as keys (j)\n",
    "        alpha[0] = {}\n",
    "        for label in self.labels:\n",
    "            alpha[0][label] = self.compute_psi(sentence[0], label, 'start')\n",
    "            \n",
    "        # inductive case\n",
    "        t = 1\n",
    "        while t <= T - 1:\n",
    "            alpha[t] = {}\n",
    "            for label in self.labels:\n",
    "                s = 0.0\n",
    "                for prev_label in self.labels:\n",
    "                    s += self.compute_psi(sentence[t-1], label, prev_label) * alpha[t-1][prev_label]\n",
    "                    \n",
    "                alpha[t][label] = s\n",
    "            \n",
    "            t += 1\n",
    "            \n",
    "        return alpha\n",
    "        \n",
    "        \n",
    "    def backward_variables(self, sentence):\n",
    "        '''\n",
    "        Compute the backward variables for a given sentence.\n",
    "        Parameters: sentence: list of strings representing a sentence.\n",
    "        Returns: data structure containing the matrix of backward variables\n",
    "        '''\n",
    "        \n",
    "        T = len(sentence)\n",
    "        # beta is indexed from 0 to T - 1 to be consistent with the array indexing\n",
    "        beta = {}        \n",
    "        \n",
    "        # initialization\n",
    "        # this dictionary contains labels as keys (j)\n",
    "        beta[T-1] = {}\n",
    "        for label in self.labels:\n",
    "            beta[T-1][label] = 1\n",
    "            \n",
    "        # inductive case\n",
    "        t = T - 2\n",
    "        while t >= 0:\n",
    "            beta[t] = {}\n",
    "            for label in self.labels:\n",
    "                s = 0.0\n",
    "                for prev_label in self.labels:\n",
    "                    s += self.compute_psi(sentence[t+1], label, prev_label) * beta[t+1][prev_label]\n",
    "                    \n",
    "                beta[t][label] = s\n",
    "            \n",
    "            t -= 1\n",
    "            \n",
    "        return beta\n",
    "        \n",
    "    \n",
    "    # Exercise 1 b) ###################################################################\n",
    "    def compute_z(self, alpha_final):\n",
    "        '''\n",
    "        Compute the partition function Z(x).\n",
    "        Parameters: alpha_final: final values of forward variables\n",
    "        Returns: float;\n",
    "        '''\n",
    "\n",
    "        return sum(alpha_final.values())\n",
    "        \n",
    "            \n",
    "    # Exercise 1 c) ###################################################################\n",
    "    def marginal_probability(self, sentence, t, y_t, y_t_minus_one):\n",
    "        '''\n",
    "        Compute the marginal probability of the labels given by y_t and y_t_minus_one given a sentence.\n",
    "        Parameters: sentence: list of strings representing a sentence.\n",
    "                    y_t: element of the set 'self.labels'; label assigned to the word at position t\n",
    "                    y_t_minus_one: element of the set 'self.labels'; label assigned to the word at position t-1\n",
    "        Returns: float: probability;\n",
    "        '''\n",
    "        \n",
    "        assert t >= 0\n",
    "        assert t < len(sentence)\n",
    "        assert y_t in self.labels\n",
    "        assert y_t_minus_one in self.labels\n",
    "        \n",
    "        alpha = self.forward_variables(sentence)\n",
    "        beta = self.backward_variables(sentence)\n",
    "        z = self.compute_z(alpha[len(sentence)-1])\n",
    "        \n",
    "        if t == 0:\n",
    "            if y_t_minus_one != 'start':\n",
    "                # return zero if the start lable is not start\n",
    "                return 0.0\n",
    "            return 1/z * self.compute_psi(sentence[t], y_t, y_t_minus_one) * beta[t][y_t]\n",
    "        else:\n",
    "            return 1/z * alpha[t-1][y_t_minus_one] * self.compute_psi(sentence[t], y_t, y_t_minus_one) * beta[t][y_t]\n",
    "    \n",
    "    \n",
    "    # Exercise 1 d) ###################################################################\n",
    "    def expected_feature_count(self, sentence, feature):\n",
    "        '''\n",
    "        Compute the expected feature count for the feature referenced by 'feature'\n",
    "        Parameters: sentence: list of strings representing a sentence.\n",
    "                    feature: a feature; element of the set 'self.features'\n",
    "        Returns: float;\n",
    "        '''\n",
    "        \n",
    "        expected_count = 0.0\n",
    "        for t, token in enumerate(sentence):\n",
    "            for prev_label in self.labels:\n",
    "                for label in self.labels:\n",
    "                    features = build_features(token, label, prev_label)\n",
    "                    if feature in features:\n",
    "                        expected_count += self.marginal_probability(sentence, t, label, prev_label)\n",
    "                            \n",
    "        return expected_count\n",
    "    \n",
    "    def full_expected_feature_count(self, sentence):\n",
    "        feature_count = OrderedDict()\n",
    "        \n",
    "        # use theta.keys instead of features because it is ordered\n",
    "        for feature in self.theta.keys():\n",
    "            feature_count[feature] = self.expected_feature_count(sentence, feature)\n",
    "            \n",
    "        return orderedDict2array(feature_count)    \n",
    "    \n",
    "#     def _full_expected_feature_count(self, sentence):\n",
    "#         feature_count = OrderedDict()\n",
    "        \n",
    "#         for feature in self.theta.keys():\n",
    "#             feature_count[feature] = 0.0\n",
    "\n",
    "#         for t, token in enumerate(sentence):\n",
    "#             for prev_label in self.labels:\n",
    "#                 for label in self.labels:\n",
    "#                     features = build_features(token, label, prev_label)\n",
    "#                     for feature in features:\n",
    "#                         # if the combination is a know feature\n",
    "#                         if feature in feature_count:\n",
    "#                             # sum up its marginal probability\n",
    "#                             feature_count[feature] += self.marginal_probability(sentence, t, label, prev_label)\n",
    "            \n",
    "#         return orderedDict2array(feature_count)\n",
    "    \n",
    "    def full_empirical_feature_count(self, sentence):\n",
    "        \n",
    "        # initialize feature count to zero\n",
    "        feature_count = OrderedDict()\n",
    "        # use theta.keys instead of features because it is ordered\n",
    "        for feature in self.theta.keys():\n",
    "            feature_count[feature] = 0\n",
    "            \n",
    "        # count features\n",
    "        prev_label = 'start'\n",
    "        for token, label in sentence:\n",
    "            observed_features = build_features(token, label, prev_label)\n",
    "            \n",
    "            for feature in observed_features:\n",
    "                assert feature in feature_count, feature\n",
    "                feature_count[feature] += 1\n",
    "                    \n",
    "            prev_label = label\n",
    "            \n",
    "        return orderedDict2array(feature_count)\n",
    "        \n",
    "    \n",
    "    # Exercise 1 e) ###################################################################\n",
    "    def train(self, num_iterations, learning_rate=0.01):\n",
    "        '''\n",
    "        Method for training the CRF.\n",
    "        Parameters: num_iterations: int; number of training iterations\n",
    "                    learning_rate: float\n",
    "        '''\n",
    "        \n",
    "        sentence = random.choice(self.corpus)\n",
    "        # tokens only\n",
    "        x = list(map(lambda pair: pair[0], sentence))\n",
    "        \n",
    "        empirical_count = self.full_empirical_feature_count(sentence)\n",
    "        expected_count = self.full_expected_feature_count(x)\n",
    "        \n",
    "        gradient = empirical_count - expected_count\n",
    "        \n",
    "        for i, key in enumerate(self.theta.keys()):\n",
    "            self.theta[key] += learning_rate * gradient[i]\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Exercise 2 ###################################################################\n",
    "    def most_likely_label_sequence(self, sentence):\n",
    "        '''\n",
    "        Compute the most likely sequence of labels for the words in a given sentence.\n",
    "        Parameters: sentence: list of strings representing a sentence.\n",
    "        Returns: list of lables; each label is an element of the set 'self.labels'\n",
    "        '''\n",
    "\n",
    "        T = len(sentence) - 1\n",
    "\n",
    "        # (label, observation_index) --> log probability\n",
    "        trellis = defaultdict(lambda: -inf)\n",
    "\n",
    "        # list to keep track of the values\n",
    "        gamma_list = []\n",
    "\n",
    "        # label with maximum probability\n",
    "        current_gamma = \"\"\n",
    "        \n",
    "        # maximum logarithmic probability\n",
    "        max_probability = -inf\n",
    "        \n",
    "        for label in self.labels:\n",
    "            trellis[(label, 0)] = full_log(self.compute_psi(sentence[0], label, 'start'))\n",
    "            \n",
    "            if trellis[(label, 0)] > max_probability:\n",
    "                max_probability = trellis[label, 0]\n",
    "                current_gamma = label\n",
    "\n",
    "        gamma_list.append(current_gamma)\n",
    "\n",
    "        t = 1\n",
    "        while t <= T:\n",
    "            max_probability = -inf\n",
    "\n",
    "            # label of the current delta\n",
    "            for label_j in self.labels:\n",
    "                delta_j = -inf\n",
    "                \n",
    "                # label of the previous delta\n",
    "                for label_i in self.labels:                    \n",
    "                    delta_j_candidate = trellis[label_i,t-1] + full_log(self.compute_psi(sentence[t], label_j, label_i))\n",
    "\n",
    "                    # consider the maximum\n",
    "                    if delta_j_candidate > delta_j:\n",
    "                        delta_j = delta_j_candidate\n",
    "                        current_gamma = label_j\n",
    "\n",
    "                trellis[label_j, t] = delta_j\n",
    "\n",
    "            gamma_list.append(current_gamma)\n",
    "            t += 1\n",
    "\n",
    "        # print(trellis)\n",
    "        return gamma_list\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'DT'), ('Lorillard', 'NNP'), ('spokewoman', 'NN'), ('said', 'VBD'), ('This', 'DT'), ('is', 'VBZ'), ('an', 'DT'), ('old', 'JJ'), ('story', 'NN')]\n",
      "[('IBM', 'NNP'), (\"'s\", 'POS'), ('750', 'CD'), ('million', 'CD'), ('*U*', '-NONE-'), ('debenture', 'NN'), ('offering', 'NN'), ('dominated', 'VBD'), ('activity', 'NN'), ('in', 'IN'), ('the', 'DT'), ('corporate', 'JJ'), ('debt', 'NN'), ('market', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "corpus = import_corpus('corpus_pos.txt')\n",
    "\n",
    "# random.shuffle(corpus)\n",
    "\n",
    "split_limit = int(len(corpus) * 0.8)\n",
    "training_corpus = corpus[:split_limit]\n",
    "test_corpus = corpus[split_limit:]\n",
    "\n",
    "print(training_corpus[0])\n",
    "print(test_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "{'CC', '-NONE-', 'VBP', 'VBD', 'NN', 'VBN', 'NNS', 'RB', 'start', 'CD', 'VBZ', 'VBG', 'DT', 'JJ', 'PRP', 'TO', 'POS', 'IN', 'NNP'}\n"
     ]
    }
   ],
   "source": [
    "lcrf = LinearChainCRF()\n",
    "lcrf.initialize(training_corpus[:5])\n",
    "\n",
    "print(orderedDict2array(lcrf.theta))\n",
    "print(lcrf.labels)\n",
    "# print(lcrf.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'Lorillard', 'spokewoman', 'said', 'This', 'is', 'an', 'old', 'story']\n",
      "{'CC': 1.0, '-NONE-': 1.0, 'VBP': 1.0, 'VBD': 1.0, 'NN': 1.0, 'VBN': 1.0, 'NNS': 1.0, 'RB': 1.0, 'start': 1.0, 'CD': 1.0, 'VBZ': 1.0, 'VBG': 1.0, 'DT': 7.38905609893065, 'JJ': 1.0, 'PRP': 2.718281828459045, 'TO': 1.0, 'POS': 1.0, 'IN': 2.718281828459045, 'NNP': 2.718281828459045}\n",
      "{'CC': 1, '-NONE-': 1, 'VBP': 1, 'VBD': 1, 'NN': 1, 'VBN': 1, 'NNS': 1, 'RB': 1, 'start': 1, 'CD': 1, 'VBZ': 1, 'VBG': 1, 'DT': 1, 'JJ': 1, 'PRP': 1, 'TO': 1, 'POS': 1, 'IN': 1, 'NNP': 1}\n"
     ]
    }
   ],
   "source": [
    "sentence = training_corpus[0]\n",
    "x = list(map(lambda pair: pair[0], sentence))\n",
    "print(x)\n",
    "\n",
    "alpha = lcrf.forward_variables(x)\n",
    "print(alpha[0])\n",
    "\n",
    "beta = lcrf.backward_variables(x)\n",
    "print(beta[len(sentence)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10234784412549.58\n"
     ]
    }
   ],
   "source": [
    "z = lcrf.compute_z(alpha[len(sentence)-1])\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20351840033930563\n"
     ]
    }
   ],
   "source": [
    "print(lcrf.expected_feature_count(x, 't=Lorillard/l=NNP'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "print(len(sentence))\n",
    "print(lcrf.full_empirical_feature_count(sentence).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.42653902455379\n",
      "0:00:10.622554\n"
     ]
    }
   ],
   "source": [
    "then = datetime.now()\n",
    "print(lcrf.full_expected_feature_count(x).sum())\n",
    "print(datetime.now() - then)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:10.601994\n"
     ]
    }
   ],
   "source": [
    "then = datetime.now()\n",
    "lcrf.train(1)\n",
    "print(datetime.now() - then)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IBM', \"'s\", '750', 'million', '*U*', 'debenture', 'offering', 'dominated', 'activity', 'in', 'the', 'corporate', 'debt', 'market']\n",
      "['DT', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP']\n"
     ]
    }
   ],
   "source": [
    "sentence = test_corpus[0]\n",
    "x = list(map(lambda pair: pair[0], sentence))\n",
    "\n",
    "print(x)\n",
    "\n",
    "y = lcrf.most_likely_label_sequence(x)\n",
    "print(y)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
