{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanations regarding the evaluation are at the bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# flatten lists\n",
    "from functools import reduce\n",
    "from operator import iconcat\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "'''\n",
    "This function can be used for importing the corpus.\n",
    "Parameters: path_to_file: string; path to the file containing the corpus\n",
    "Returns: list of list; the first layer list contains the sentences of the corpus;\n",
    "    the second layer list contains tuples (token,label) representing a labelled sentence\n",
    "'''\n",
    "def import_corpus(path_to_file):\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    f = open(path_to_file)\n",
    "\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line: break\n",
    "\n",
    "        line = line.strip()\n",
    "        if len(line) == 0:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "            continue\n",
    "\n",
    "        parts = line.split(' ')\n",
    "        sentence.append((parts[0], parts[-1]))\n",
    "\n",
    "    f.close()\n",
    "    return sentences\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MaxEntModel(object):\n",
    "    # training corpus\n",
    "    corpus = None\n",
    "    \n",
    "    # (numpy) array containing the parameters of the model\n",
    "    # has to be initialized by the method 'initialize'\n",
    "    theta = None\n",
    "    \n",
    "    # dictionary containing all possible features of a corpus and their corresponding index;\n",
    "    # has to be set by the method 'initialize'; hint: use a Python dictionary\n",
    "    feature_indices = None\n",
    "    \n",
    "    # set containing a list of possible lables\n",
    "    # has to be set by the method 'initialize'\n",
    "    labels = None\n",
    "    \n",
    "    \n",
    "    cond_normalization_factor_memoization = {}\n",
    "    \n",
    "    # number of words used in training so far\n",
    "    training_words_number = 0\n",
    "    \n",
    "    START_LABEL = '<START>'\n",
    "    \n",
    "    \n",
    "    # Exercise 1 a) ###################################################################\n",
    "    def initialize(self, corpus):\n",
    "        '''\n",
    "        Initialize the maximun entropy model, i.e., build the set of all features, the set of all labels\n",
    "        and create an initial array 'theta' for the parameters of the model.\n",
    "        Parameters: corpus: list of list representing the corpus, returned by the function 'import_corpus'\n",
    "        '''\n",
    "        self.corpus = corpus\n",
    "\n",
    "        # get set of words\n",
    "        words = list(map(lambda sentence: list(map(lambda pair: pair[0], sentence)), corpus))\n",
    "        words = set(reduce(iconcat, words, []))\n",
    "\n",
    "        # get set of labels\n",
    "        labels = list(map(lambda sentence: list(map(lambda pair: pair[1], sentence)), corpus))\n",
    "        labels = set(reduce(iconcat, labels, []))\n",
    "        labels.add(MaxEntModel.START_LABEL)\n",
    "        \n",
    "        self.labels = labels        \n",
    "        \n",
    "        self.feature_indices = {}\n",
    "\n",
    "        index = 0\n",
    "\n",
    "        for word in words:\n",
    "            for tag in labels:\n",
    "                self.feature_indices[(word, tag)] = index\n",
    "                index += 1\n",
    "        \n",
    "        for prev in labels:\n",
    "            for curr in labels:\n",
    "                self.feature_indices[(prev, curr)] = index\n",
    "                index += 1\n",
    "        \n",
    "        self.theta = np.ones(index)\n",
    "    \n",
    "    \n",
    "    # Exercise 1 b) ###################################################################\n",
    "    def get_active_features(self, word, label, prev_label):\n",
    "        '''\n",
    "        Compute the vector of active features.\n",
    "        Parameters: word: string; a word at some position i of a given sentence\n",
    "                    label: string; a label assigned to the given word\n",
    "                    prev_label: string; the label of the word at position i-1\n",
    "        Returns: (numpy) array containing only zeros and ones.\n",
    "        '''\n",
    "        \n",
    "        active_features = {(word, label), (prev_label, label)}\n",
    "        \n",
    "        active_features_vector = np.zeros(len(self.theta))\n",
    "        \n",
    "        for feature in active_features:\n",
    "            # if the feature is present in the training features\n",
    "            if feature in self.feature_indices:\n",
    "                active_features_vector[self.feature_indices[feature]] = 1\n",
    "        \n",
    "        return active_features_vector\n",
    "\n",
    "\n",
    "\n",
    "    # Exercise 2 a) ###################################################################\n",
    "    def cond_normalization_factor(self, word, prev_label):\n",
    "        '''\n",
    "        Compute the normalization factor 1/Z(x_i).\n",
    "        Parameters: word: string; a word x_i at some position i of a given sentence\n",
    "                    prev_label: string; the label of the word at position i-1\n",
    "        Returns: float\n",
    "        '''\n",
    "        \n",
    "        # we use memoization to avoid recalculating the normalization factor\n",
    "        if (word, prev_label) in self.cond_normalization_factor_memoization:\n",
    "            return self.cond_normalization_factor_memoization[(word, prev_label)]\n",
    "        \n",
    "        z = 0\n",
    "        for label in self.labels:\n",
    "            exponent = np.dot(self.theta, self.get_active_features(word, label, prev_label))\n",
    "            z += math.e ** exponent\n",
    "            \n",
    "        factor = 1 / z\n",
    "        \n",
    "        self.cond_normalization_factor_memoization[(word, prev_label)] = factor\n",
    "        \n",
    "        return factor\n",
    "        \n",
    "    \n",
    "    # Exercise 2 b) ###################################################################\n",
    "    def conditional_probability(self, label, word, prev_label):\n",
    "        '''\n",
    "        Compute the conditional probability of a label given a word x_i.\n",
    "        Parameters: label: string; we are interested in the conditional probability of this label\n",
    "                    word: string; a word x_i some position i of a given sentence\n",
    "                    prev_label: string; the label of the word at position i-1\n",
    "        Returns: float\n",
    "        '''\n",
    "        \n",
    "        exponent = np.dot(self.theta, self.get_active_features(word, label, prev_label))\n",
    "        return self.cond_normalization_factor(word, prev_label) * math.e ** exponent    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Exercise 3 a) ###################################################################\n",
    "    def empirical_feature_count(self, word, label, prev_label):\n",
    "        '''\n",
    "        Compute the empirical feature count given a word, the actual label of this word and the label of the previous word.\n",
    "        Parameters: word: string; a word x_i some position i of a given sentence\n",
    "                    label: string; the actual label of the given word\n",
    "                    prev_label: string; the label of the word at position i-1\n",
    "        Returns: (numpy) array containing the empirical feature count\n",
    "        '''\n",
    "        \n",
    "        return self.get_active_features(word, label, prev_label)    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Exercise 3 b) ###################################################################\n",
    "    def expected_feature_count(self, word, prev_label):\n",
    "        '''\n",
    "        Compute the expected feature count given a word, the label of the previous word and the parameters of the current model\n",
    "        (see variable theta)\n",
    "        Parameters: word: string; a word x_i some position i of a given sentence\n",
    "                    prev_label: string; the label of the word at position i-1\n",
    "        Returns: (numpy) array containing the expected feature count\n",
    "        '''\n",
    "        \n",
    "        expected = np.zeros(len(self.theta))\n",
    "        \n",
    "        for label in self.labels:\n",
    "            active_features = self.get_active_features(word, label, prev_label)\n",
    "            conditional_probability = self.conditional_probability(label, word, prev_label)\n",
    "            \n",
    "            expected = expected + np.dot(conditional_probability, active_features)\n",
    "            \n",
    "        return expected\n",
    "    \n",
    "    \n",
    "    # Exercise 4 a) ###################################################################\n",
    "    def parameter_update(self, word, label, prev_label, learning_rate):\n",
    "        '''\n",
    "        Do one learning step.\n",
    "        Parameters: word: string; a randomly selected word x_i at some position i of a given sentence\n",
    "                    label: string; the actual label of the selected word\n",
    "                    prev_label: string; the label of the word at position i-1\n",
    "                    learning_rate: float\n",
    "        '''\n",
    "        \n",
    "        empirical_count = self.empirical_feature_count(word, label, prev_label)\n",
    "        expected_count = self.expected_feature_count(word, prev_label)\n",
    "        \n",
    "        self.theta += learning_rate * (empirical_count - expected_count)\n",
    "        self.cond_normalization_factor_memoization = {}\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Exercise 4 b) ###################################################################\n",
    "    def train(self, number_iterations, learning_rate=0.1):\n",
    "        '''\n",
    "        Implement the training procedure.\n",
    "        Parameters: number_iterations: int; number of parameter updates to do\n",
    "                    learning_rate: float\n",
    "        '''\n",
    "                    \n",
    "        for sentence in self.corpus:\n",
    "            last_label = MaxEntModel.START_LABEL\n",
    "            for word, label in sentence:\n",
    "                self.parameter_update(word, label, last_label, learning_rate)\n",
    "                self.training_words_number += 1\n",
    "                number_iterations -= 1\n",
    "                if number_iterations == 0:\n",
    "                    return\n",
    "    \n",
    "    \n",
    "    # Exercise 4 c) ###################################################################\n",
    "    def predict(self, word, prev_label):\n",
    "        '''\n",
    "        Predict the most probable label of the word referenced by 'word'\n",
    "        Parameters: word: string; a word x_i at some position i of a given sentence\n",
    "                    prev_label: string; the label of the word at position i-1\n",
    "        Returns: string; most probable label\n",
    "        '''\n",
    "        most_probable_lable = None\n",
    "        greatest_probability = 0\n",
    "        \n",
    "        for label in self.labels:\n",
    "            label_probability = self.conditional_probability(label, word, prev_label)\n",
    "            if label_probability >= greatest_probability:\n",
    "                greatest_probability = label_probability\n",
    "                most_probable_lable = label\n",
    "                \n",
    "        return most_probable_lable\n",
    "    \n",
    "    \n",
    "    # Exercise 5 a) ###################################################################\n",
    "    def empirical_feature_count_batch(self, sentences):\n",
    "        '''\n",
    "        Predict the empirical feature count for a set of sentences\n",
    "        Parameters: sentences: list; a list of sentences; should be a sublist of the list returnd by 'import_corpus'\n",
    "        Returns: (numpy) array containing the empirical feature count\n",
    "        '''\n",
    "        empirical_feature_sum = np.zeros(len(self.feature_indices))\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            prev_label = MaxEntModel.START_LABEL\n",
    "            for word, label in sentence:\n",
    "                empirical_feature_sum = empirical_feature_sum + self.empirical_feature_count(word, label, prev_label)\n",
    "                prev_label = label\n",
    "                \n",
    "        return empirical_feature_sum\n",
    "    \n",
    "    \n",
    "    # Exercise 5 a) ###################################################################\n",
    "    def expected_feature_count_batch(self, sentences):\n",
    "        '''\n",
    "        Predict the expected feature count for a set of sentences\n",
    "        Parameters: sentences: list; a list of sentences; should be a sublist of the list returnd by 'import_corpus'\n",
    "        Returns: (numpy) array containing the expected feature count\n",
    "        '''\n",
    "        \n",
    "        expected_feature_sum = np.zeros(len(self.feature_indices))\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            prev_label = MaxEntModel.START_LABEL\n",
    "            for word, label in sentence:\n",
    "                expected_feature_sum = expected_feature_sum + self.expected_feature_count(word, prev_label)\n",
    "                prev_label = label\n",
    "                \n",
    "        return expected_feature_sum\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Exercise 5 b) ###################################################################\n",
    "    def train_batch(self, number_iterations, batch_size, learning_rate=0.1):\n",
    "        '''\n",
    "        Implement the training procedure which uses 'batch_size' sentences from to training corpus\n",
    "        to compute the gradient.\n",
    "        Parameters: number_iterations: int; number of parameter updates to do\n",
    "                    batch_size: int; number of sentences to use in each iteration\n",
    "                    learning_rate: float\n",
    "        '''\n",
    "        \n",
    "        for _ in range(number_iterations):\n",
    "            for sentence in random.sample(self.corpus, batch_size):\n",
    "                last_label = MaxEntModel.START_LABEL\n",
    "                for word, label in sentence:\n",
    "                    self.parameter_update(word, label, last_label, learning_rate)\n",
    "                    self.training_words_number += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = import_corpus('corpus_pos.txt')\n",
    "# print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Theta| ==  571458\n",
      "Labels:  {'TO', '<START>', 'WP$', 'RBS', 'WRB', ':', 'NNPS', '-RRB-', '$', 'PDT', 'RP', 'WDT', '.', 'VBG', 'RB', 'VBZ', 'JJR', 'NN', 'JJS', 'EX', 'JJ', 'NNP', 'VBN', 'RBR', 'VBD', 'PRP', ',', 'NNS', \"''\", '-NONE-', 'MD', 'IN', 'VB', 'FW', 'CD', 'UH', 'LS', 'VBP', 'POS', 'SYM', 'PRP$', '-LRB-', '``', 'CC', 'DT', 'WP'}\n"
     ]
    }
   ],
   "source": [
    "maxent = MaxEntModel()\n",
    "maxent.initialize(corpus)\n",
    "print('|Theta| == ', len(maxent.theta))\n",
    "print('Labels: ', maxent.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_word, test_label, test_prev_label = ('starters', 'NNS', 'IN')\n",
    "s = np.sum(maxent.get_active_features(test_word, test_label, test_prev_label))\n",
    "if s != 2:\n",
    "    print('Error: ', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0029420713747089714\n"
     ]
    }
   ],
   "source": [
    "z = maxent.cond_normalization_factor(test_word, test_prev_label)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.021739130434782605\n"
     ]
    }
   ],
   "source": [
    "p = maxent.conditional_probability(\"-NONE-\", test_word, test_prev_label)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.]\n",
      "2.0\n",
      "1.9999999999999996\n"
     ]
    }
   ],
   "source": [
    "enp = maxent.empirical_feature_count(test_word, test_label, test_prev_label)\n",
    "exp = maxent.expected_feature_count(test_word, test_prev_label)\n",
    "print(enp, exp)\n",
    "print(np.sum(enp))\n",
    "print(np.sum(exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "571458.0\n"
     ]
    }
   ],
   "source": [
    "maxent.train(10)\n",
    "print(np.sum(maxent.theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WP\n"
     ]
    }
   ],
   "source": [
    "print(maxent.predict(test_word, test_prev_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.0\n"
     ]
    }
   ],
   "source": [
    "emp = maxent.empirical_feature_count_batch(corpus[:2])\n",
    "print(np.sum(emp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.0\n"
     ]
    }
   ],
   "source": [
    "exp = maxent.expected_feature_count_batch(corpus[:2])\n",
    "print(np.sum(exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "571458.0\n"
     ]
    }
   ],
   "source": [
    "maxent.train_batch(1, 1)\n",
    "print(np.sum(maxent.theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, test_set):\n",
    "    hits = 0\n",
    "    total = 0\n",
    "\n",
    "    for sentence in test_set:\n",
    "        prev_label = MaxEntModel.START_LABEL\n",
    "        for word, label in sentence:\n",
    "            prediction = model.predict(word, prev_label)\n",
    "        \n",
    "            total += 1\n",
    "            if prediction == label:\n",
    "                hits += 1\n",
    "            \n",
    "            prev_label = label\n",
    "\n",
    "    return hits/total\n",
    "    \n",
    "\n",
    "# Exercise 5 c) ###################################################################\n",
    "    \n",
    "# split the corpus into training and test set\n",
    "random.shuffle(corpus)\n",
    "training_set = corpus[:int(len(corpus) * 0.90)]\n",
    "test_set = corpus[int(len(corpus) * 0.90):]\n",
    "\n",
    "A = MaxEntModel()\n",
    "A.initialize(training_set)\n",
    "\n",
    "B = MaxEntModel()\n",
    "B.initialize(training_set)\n",
    "\n",
    "N = 20\n",
    "A.train(N)\n",
    "B.train_batch(N, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.028368794326241134\n",
      "0.3617021276595745\n"
     ]
    }
   ],
   "source": [
    "accuracy_A = get_accuracy(A, test_set[:10])\n",
    "accuracy_B = get_accuracy(B, test_set[:10])\n",
    "\n",
    "print(accuracy_A)\n",
    "print(accuracy_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key savefig.frameon in file /home/michele/miniconda3/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 421 ('savefig.frameon : True')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key verbose.level in file /home/michele/miniconda3/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 472 ('verbose.level  : silent      # one of silent, helpful, debug, debug-annoying')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key verbose.fileo in file /home/michele/miniconda3/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 473 ('verbose.fileo  : sys.stdout  # a log filename, sys.stdout or sys.stderr')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "In /home/michele/miniconda3/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/michele/miniconda3/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/michele/miniconda3/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /home/michele/miniconda3/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/michele/miniconda3/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/michele/miniconda3/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/michele/miniconda3/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/michele/miniconda3/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0014184397163120566\n",
      "0.0007914707388612134\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(accuracy_A/A.training_words_number)\n",
    "print(accuracy_B/B.training_words_number)\n",
    "\n",
    "results_list = [accuracy_A/A.training_words_number, accuracy_B/B.training_words_number]\n",
    "    \n",
    "plt.bar(['A', 'B'], results_list)\n",
    "plt.ylabel('Accuracy against number of words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would expect the accuracy against number of words to be more or less the same for the two models. I guess this doesn't happen in the previous evaluation because the input data is not enough to have quality results.\n",
    "\n",
    "The only true difference of the two training methods is that B (considering a large number of iterations) will consider the same sentences more than once, because the sentences are sampled randomly.\n",
    "\n",
    "I couldn't use a larger amount of data because of the time complexity of this implementation of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
